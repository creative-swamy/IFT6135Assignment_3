{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE-problem.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_kO6Pyf_BrWC",
        "colab_type": "code",
        "outputId": "8e9e3ed2-d4bc-493b-8b66-9706c0db705d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from torchvision.datasets import utils\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.nn.modules import upsampling\n",
        "from torch.functional import F\n",
        "from torch.optim import Adam\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "BATCH_SIZE=128\n",
        "\n",
        "def get_data_loader(dataset_location, batch_size):\n",
        "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
        "    # start processing\n",
        "    def lines_to_np_array(lines):\n",
        "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
        "    splitdata = []\n",
        "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
        "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
        "        filepath = os.path.join(dataset_location, filename)\n",
        "        utils.download_url(URL + filename, dataset_location)\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "        x = lines_to_np_array(lines).astype('float32')\n",
        "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
        "        # pytorch data loader\n",
        "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
        "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
        "        splitdata.append(dataset_loader)\n",
        "    return splitdata\n",
        "  \n",
        "train_loader, valid_loader, test_loader = get_data_loader(\"binarized_mnist\", BATCH_SIZE)\n",
        "  \n",
        "#torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "device = torch.device(\"cuda\" )\n",
        "\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/78400000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_train.amat to binarized_mnist/binarized_mnist_train.amat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "78405632it [00:08, 9067184.59it/s]                              \n",
            "  0%|          | 0/15680000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_valid.amat to binarized_mnist/binarized_mnist_valid.amat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15687680it [00:02, 7473919.39it/s]                              \n",
            "  0%|          | 0/15680000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_test.amat to binarized_mnist/binarized_mnist_test.amat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15687680it [00:02, 5831053.22it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.0.1.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FOjkYGpU9ENF",
        "colab_type": "code",
        "outputId": "5545974e-780d-41ec-e58a-7761dc87a130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2485
        }
      },
      "cell_type": "code",
      "source": [
        "epochs=20\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.e1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.e2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.e3 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.e4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.e5 = nn.Conv2d(64, 256, kernel_size=5)\n",
        "        \n",
        "        self.e61 = nn.Linear(256, 100)\n",
        "        self.e62= nn.Linear(256,100)\n",
        "        \n",
        "        self.d1= nn.Linear(100,256)\n",
        "        self.conv1= nn.Conv2d(256, 64, 5, padding=4)\n",
        "        self.upsample= nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "        self.conv2= nn.Conv2d(64, 32, kernel_size=3, padding=2)\n",
        "        \n",
        "        self.conv3= nn.Conv2d(32, 16, kernel_size=3, padding= 2)\n",
        "       \n",
        "        self.conv4= nn.Conv2d(16,1 , kernel_size=3, padding=2)\n",
        "       \n",
        "        \n",
        "    def encoder(self, x):\n",
        "   \n",
        "        h1= F.elu(self.e1(x))\n",
        "        h3= F.elu( self.e3(self.e2(h1)) )\n",
        "        h5= F.elu(self.e5(self.e4(h3)))\n",
        "      \n",
        "        h6=h5.view(-1, 256)\n",
        "        h61= self.e61(h6)\n",
        "        h62= self.e62(h6)\n",
        "        \n",
        "        return h61, h62\n",
        "        #returning the mean and log variance\n",
        "        \n",
        "    def reparametrize(self, mu , log_variance):\n",
        "       # print('original size mu: ', mu.size())\n",
        "        sigma= torch.exp(0.5 * log_variance)\n",
        "        e= torch.randn_like(sigma)\n",
        "        z= mu+ e*sigma\n",
        "        return z\n",
        "    \n",
        "    #K samples. Sends K Z valyes, E samples\n",
        "    def reparametrize_iwae(self,K, mu , log_variance):\n",
        "       \n",
        "        sigma= torch.exp(0.5 * log_variance)\n",
        "        z_samples= []\n",
        "        e_samples=[]\n",
        "        #generating K samples for the IWAE\n",
        "        for i in range(K):\n",
        "          e= torch.randn_like(sigma)        \n",
        "          z= mu+ e*sigma\n",
        " \n",
        "          z_samples.append(z)\n",
        "          e_samples.append(e) \n",
        "        \n",
        "        return torch.stack(z_samples),mu,sigma, torch.stack(e_samples)\n",
        "    \n",
        "    \n",
        "    def decoder(self, z):\n",
        "        \n",
        "        h1= F.elu(self.d1(z))\n",
        "        \n",
        "        h1= h1.view(h1.size(0), 256, 1, 1)\n",
        "             \n",
        "        h2= F.elu(self.conv1(h1))\n",
        "        \n",
        "        h3=self.upsample(h2)\n",
        "      \n",
        "        h4= F.elu(self.conv2(h3))\n",
        "        h5= self.upsample(h4)\n",
        "      \n",
        "        h6= F.elu(self.conv3(h5))\n",
        "      \n",
        "        h7= self.conv4(h6)\n",
        "      \n",
        "        h10= F.sigmoid(h7)\n",
        "        \n",
        "        return h10\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_variance= self.encoder(x)\n",
        "        z= self.reparametrize(mu,log_variance)\n",
        "\n",
        "        return self.decoder(z), mu, log_variance\n",
        "    \n",
        "    #evaluate within one mini-batch\n",
        "    def evaluate_IWAE_within_one_mini_batch(self,K, inputs,x_tilda, mu, log_variance):\n",
        "        \n",
        "        z_samples, mu_h1, sigma, eps_samples = self.reparametrize_iwae(K, mu , log_variance)\n",
        "  \n",
        "        log_Qz_x = torch.sum(-0.5*(eps_samples)**2 - torch.log(sigma), -1)       \n",
        "        #K,128. first dimension basically contains how many samples taken\n",
        "        \n",
        "        p_set=[]\n",
        "        for z in z_samples:      \n",
        "          p = self.decoder(z)\n",
        "          p_set.append(p)\n",
        "        p_x_given_z=torch.stack(p_set)\n",
        "        \n",
        "        log_P_z = torch.sum(-0.5*z_samples**2, -1)\n",
        "        #K, 128\n",
        "              \n",
        "        cross_entropy = F.binary_cross_entropy(x_tilda, inputs, reduction='none')\n",
        "        cross_entropy= cross_entropy.sum(-1).sum(-1).sum(-1)\n",
        "        \n",
        "        #doing normalisation\n",
        "        log_weight = log_P_z - cross_entropy - log_Qz_x\n",
        "        log_weight = log_weight - torch.max(log_weight, 0)[0]\n",
        "        weight = torch.exp(log_weight)\n",
        "        weight = weight / torch.sum(weight, 0)\n",
        "        weight = Variable(weight.data, requires_grad = False)\n",
        "        \n",
        "        loss = (torch.sum(weight * (log_P_z - cross_entropy - log_Qz_x) , 0))\n",
        "       # print('loss dimension is: ', loss.size())\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #some inspiration taken from other codes\n",
        "    #Evaluation with IWAE, across the whole dataset\n",
        "    def evaluate_IWAE(self,K, inputs,x_tilda, mu, log_variance):\n",
        "        \n",
        "        z_samples, mu_h1, sigma, eps_samples = self.reparametrize_iwae(K, mu , log_variance)\n",
        "  \n",
        "        log_Qz_x = torch.sum(-0.5*(eps_samples)**2 - torch.log(sigma), -1)       \n",
        "        #K,128. first dimension basically contains how many samples taken\n",
        "        \n",
        "        p_set=[]\n",
        "        for z in z_samples:      \n",
        "          p = self.decoder(z)\n",
        "          p_set.append(p)\n",
        "        p_x_given_z=torch.stack(p_set)\n",
        "        \n",
        "        log_P_z = torch.sum(-0.5*z_samples**2, -1)\n",
        "        #K, 128\n",
        "              \n",
        "        cross_entropy = F.binary_cross_entropy(x_tilda, inputs, reduction='none')\n",
        "        cross_entropy= cross_entropy.sum(-1).sum(-1).sum(-1)\n",
        "        \n",
        "        #doing normalisation\n",
        "        log_weight = log_P_z - cross_entropy - log_Qz_x\n",
        "        log_weight = log_weight - torch.max(log_weight, 0)[0]\n",
        "        weight = torch.exp(log_weight)\n",
        "        weight = weight / torch.sum(weight, 0)\n",
        "        weight = Variable(weight.data, requires_grad = False)\n",
        "        \n",
        "        loss = torch.sum(torch.sum(weight * (log_P_z - cross_entropy - log_Qz_x) , 0))\n",
        "        return loss      \n",
        "    \n",
        "      \n",
        "      \n",
        "model = VAE().to(device)\n",
        "optimiser= optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "def loss_function(x_tilda, x, mu, log_variance):\n",
        "  #  print('size of x tilda:',x_tilda.size(), ':x size: ', x.size() )\n",
        "    cross_entropy = F.binary_cross_entropy(x_tilda, x, reduction='sum')\n",
        "    #print('cross entropy: ', cross_entropy)\n",
        "    #cross_entropy/= BATCH_SIZE    \n",
        "\n",
        "    Kl_divergence = -0.5 * torch.sum(1 + log_variance - mu.pow(2) - log_variance.exp())\n",
        "    \n",
        "    #Kl_divergence/= BATCH_SIZE\n",
        "    \n",
        "    #print('cross entropy: ', cross_entropy,' : KL_divergence: ',Kl_divergence )\n",
        "    return cross_entropy + Kl_divergence\n",
        "\n",
        "  \n",
        "#some reference taken from the mentioned code in slack  \n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    training_loss = 0\n",
        "    for batch_index, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimiser.zero_grad()\n",
        "        reconstruction_batch, mu, log_variance = model(data)\n",
        "          \n",
        "        loss = loss_function(reconstruction_batch, data, mu, log_variance)\n",
        "        loss.backward()\n",
        "        training_loss = training_loss + loss.item()\n",
        "        optimiser.step()\n",
        "        if batch_index % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_index * len(data), len(train_loader.dataset),\n",
        "                100. * batch_index / len(train_loader),\n",
        "                -loss.item()/len(data) ))\n",
        "      \n",
        "      \n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, -training_loss / len(train_loader.dataset)))\n",
        "\n",
        "    \n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            reconstruction_batch, mu, log_variance = model(data)\n",
        "            test_loss += loss_function(reconstruction_batch, data, mu, log_variance).item()\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "   \n",
        "    print('====> Test set loss: {:.4f}'.format(-test_loss ))\n",
        "\n",
        "def test_validation(epoch):\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(valid_loader):\n",
        "            data = data.to(device)\n",
        "            reconstruction_batch, mu, log_variance = model(data)\n",
        "            validation_loss += loss_function(reconstruction_batch, data, mu, log_variance).item()\n",
        "            \n",
        "    validation_loss /= len(valid_loader.dataset)\n",
        "   \n",
        "    print('====> Validation set loss: {:.4f}'.format(-validation_loss ))\n",
        "    \n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        test_validation(epoch)\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: -552.321655\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: -212.512329\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: -171.057739\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: -167.097916\n",
            "====> Epoch: 1 Average loss: -211.6868\n",
            "====> Test set loss: -154.1568\n",
            "====> Validation set loss: -155.3712\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: -156.700912\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: -145.893539\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: -141.245819\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: -133.530609\n",
            "====> Epoch: 2 Average loss: -141.0354\n",
            "====> Test set loss: -128.7647\n",
            "====> Validation set loss: -130.1685\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: -129.216614\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: -127.551834\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: -120.235840\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: -115.707817\n",
            "====> Epoch: 3 Average loss: -123.4553\n",
            "====> Test set loss: -117.2815\n",
            "====> Validation set loss: -118.4875\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: -115.504990\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: -113.521606\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: -116.592400\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: -111.369385\n",
            "====> Epoch: 4 Average loss: -114.3390\n",
            "====> Test set loss: -110.6569\n",
            "====> Validation set loss: -111.8747\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: -110.125519\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: -107.967018\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: -111.242729\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: -108.853912\n",
            "====> Epoch: 5 Average loss: -109.2470\n",
            "====> Test set loss: -106.2933\n",
            "====> Validation set loss: -107.7165\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: -108.552505\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: -107.337959\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: -111.293961\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: -105.893005\n",
            "====> Epoch: 6 Average loss: -106.2358\n",
            "====> Test set loss: -104.0663\n",
            "====> Validation set loss: -105.4928\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: -111.530624\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: -99.183327\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: -105.269157\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: -103.514046\n",
            "====> Epoch: 7 Average loss: -104.0925\n",
            "====> Test set loss: -102.5643\n",
            "====> Validation set loss: -103.7186\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: -102.568634\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: -105.563850\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: -103.195389\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: -104.264648\n",
            "====> Epoch: 8 Average loss: -102.6700\n",
            "====> Test set loss: -101.4501\n",
            "====> Validation set loss: -102.4722\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: -104.157547\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: -104.025772\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: -103.564537\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: -98.518181\n",
            "====> Epoch: 9 Average loss: -101.3879\n",
            "====> Test set loss: -100.2766\n",
            "====> Validation set loss: -101.2412\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: -105.335510\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: -98.197189\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: -100.925934\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: -95.705307\n",
            "====> Epoch: 10 Average loss: -100.3600\n",
            "====> Test set loss: -99.1584\n",
            "====> Validation set loss: -100.0951\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: -98.704758\n",
            "Train Epoch: 11 [12800/50000 (26%)]\tLoss: -96.379173\n",
            "Train Epoch: 11 [25600/50000 (51%)]\tLoss: -102.964493\n",
            "Train Epoch: 11 [38400/50000 (77%)]\tLoss: -101.566818\n",
            "====> Epoch: 11 Average loss: -99.5359\n",
            "====> Test set loss: -98.3454\n",
            "====> Validation set loss: -99.4681\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: -104.976814\n",
            "Train Epoch: 12 [12800/50000 (26%)]\tLoss: -103.632729\n",
            "Train Epoch: 12 [25600/50000 (51%)]\tLoss: -97.060669\n",
            "Train Epoch: 12 [38400/50000 (77%)]\tLoss: -99.250412\n",
            "====> Epoch: 12 Average loss: -98.7849\n",
            "====> Test set loss: -97.6780\n",
            "====> Validation set loss: -98.6277\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: -97.077034\n",
            "Train Epoch: 13 [12800/50000 (26%)]\tLoss: -98.829300\n",
            "Train Epoch: 13 [25600/50000 (51%)]\tLoss: -94.768890\n",
            "Train Epoch: 13 [38400/50000 (77%)]\tLoss: -98.645233\n",
            "====> Epoch: 13 Average loss: -98.1062\n",
            "====> Test set loss: -97.3978\n",
            "====> Validation set loss: -98.3837\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: -96.473892\n",
            "Train Epoch: 14 [12800/50000 (26%)]\tLoss: -99.106644\n",
            "Train Epoch: 14 [25600/50000 (51%)]\tLoss: -97.901840\n",
            "Train Epoch: 14 [38400/50000 (77%)]\tLoss: -99.208893\n",
            "====> Epoch: 14 Average loss: -97.6200\n",
            "====> Test set loss: -97.4894\n",
            "====> Validation set loss: -98.3722\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: -94.983124\n",
            "Train Epoch: 15 [12800/50000 (26%)]\tLoss: -92.923988\n",
            "Train Epoch: 15 [25600/50000 (51%)]\tLoss: -95.686050\n",
            "Train Epoch: 15 [38400/50000 (77%)]\tLoss: -100.744568\n",
            "====> Epoch: 15 Average loss: -97.1923\n",
            "====> Test set loss: -96.4206\n",
            "====> Validation set loss: -97.1437\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: -96.796722\n",
            "Train Epoch: 16 [12800/50000 (26%)]\tLoss: -96.183243\n",
            "Train Epoch: 16 [25600/50000 (51%)]\tLoss: -95.505310\n",
            "Train Epoch: 16 [38400/50000 (77%)]\tLoss: -93.806824\n",
            "====> Epoch: 16 Average loss: -96.7592\n",
            "====> Test set loss: -96.0591\n",
            "====> Validation set loss: -96.9138\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: -94.427284\n",
            "Train Epoch: 17 [12800/50000 (26%)]\tLoss: -95.354912\n",
            "Train Epoch: 17 [25600/50000 (51%)]\tLoss: -97.538498\n",
            "Train Epoch: 17 [38400/50000 (77%)]\tLoss: -94.992065\n",
            "====> Epoch: 17 Average loss: -96.3452\n",
            "====> Test set loss: -95.8414\n",
            "====> Validation set loss: -96.6225\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: -97.753540\n",
            "Train Epoch: 18 [12800/50000 (26%)]\tLoss: -92.873184\n",
            "Train Epoch: 18 [25600/50000 (51%)]\tLoss: -93.969360\n",
            "Train Epoch: 18 [38400/50000 (77%)]\tLoss: -95.798241\n",
            "====> Epoch: 18 Average loss: -95.9988\n",
            "====> Test set loss: -95.7614\n",
            "====> Validation set loss: -96.7510\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: -98.757538\n",
            "Train Epoch: 19 [12800/50000 (26%)]\tLoss: -99.223587\n",
            "Train Epoch: 19 [25600/50000 (51%)]\tLoss: -96.439415\n",
            "Train Epoch: 19 [38400/50000 (77%)]\tLoss: -96.302147\n",
            "====> Epoch: 19 Average loss: -95.7216\n",
            "====> Test set loss: -95.1413\n",
            "====> Validation set loss: -95.9880\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: -94.505562\n",
            "Train Epoch: 20 [12800/50000 (26%)]\tLoss: -95.324448\n",
            "Train Epoch: 20 [25600/50000 (51%)]\tLoss: -100.388962\n",
            "Train Epoch: 20 [38400/50000 (77%)]\tLoss: -96.181976\n",
            "====> Epoch: 20 Average loss: -95.3906\n",
            "====> Test set loss: -94.9957\n",
            "====> Validation set loss: -95.7244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6V0i1xVuI0l",
        "colab_type": "code",
        "outputId": "1265da43-c5fb-4358-dbe9-c2062bd059f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "#2.1 reporting for each of the mini-batches to return to dimension[M,]\n",
        "model.eval()\n",
        "for batch_index, data in enumerate(valid_loader):\n",
        "    data = data.to(device)\n",
        "    reconstruction_batch, mu, log_variance = model(data)\n",
        "\n",
        "    loss= model.evaluate_IWAE_within_one_mini_batch(200,data,reconstruction_batch , mu , log_variance) \n",
        "    #t_loss+=loss.item()\n",
        "    #Mini_batch_evaluations.append(loss)    \n",
        "    \n",
        "    \n",
        "    if batch_index % 200 == 0:\n",
        "        print('Size of returned IWAE values for the mini-batch :',\n",
        "        loss.size() )\n",
        "        print('IWAE likelihood values within the mini-batch :',\n",
        "        loss )\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Size of returned IWAE values for the mini-batch : torch.Size([128])\n",
            "IWAE likelihood values within the mini-batch : tensor([ -85.9256,  -51.4764, -108.3658,  -71.6859,  -80.3627,  -98.8165,\n",
            "        -115.7668,  -64.4819,  -52.1680,  -56.6775,  -91.5293,  -95.2445,\n",
            "        -107.7856, -109.8123,  -66.3784,  -78.2465,  -54.7036,  -71.5919,\n",
            "         -94.3613,  -82.3873,  -69.5043,  -81.5381,  -93.9318,  -84.9353,\n",
            "        -100.1012,  -47.7049,  -82.2398,  -47.6619, -108.3008,  -85.1671,\n",
            "         -33.5919, -110.5467,  -65.0285, -104.9563,  -47.0480,  -81.4074,\n",
            "         -97.5503,  -95.4803,  -46.7834, -101.4885,  -92.6153, -110.5799,\n",
            "         -93.6362,  -90.1419,  -92.1136,  -95.4762,  -95.3591, -117.5358,\n",
            "        -114.7579,  -77.9413,  -58.0993,  -38.0706, -109.7656,  -67.9145,\n",
            "         -52.3746,  -79.8533,  -44.0732, -123.8761,  -70.2304,  -76.3815,\n",
            "         -86.1366,  -63.6862, -112.1142,  -93.1518,  -67.2565,  -75.4955,\n",
            "         -82.0293,  -88.1159,  -94.9718,  -91.6796,  -71.8233, -102.6598,\n",
            "         -43.9556,  -67.2936,  -39.1026, -108.3894,  -71.9276,  -38.7545,\n",
            "         -81.7466,  -57.5636,  -99.1872, -130.2785, -104.3325,  -82.0244,\n",
            "         -95.0672,  -89.5719,  -83.9997,  -81.2352,  -70.3057,  -87.3248,\n",
            "        -106.2197,  -71.7370,  -96.6859, -130.1443,  -88.1937,  -95.6982,\n",
            "         -84.0584,  -79.9675,  -56.2681, -104.7478, -126.4749, -101.0196,\n",
            "         -69.3672,  -82.3126,  -81.9072, -115.1422,  -84.3568,  -86.3660,\n",
            "         -78.5073,  -80.9408,  -69.3829,  -69.1754,  -94.0173,  -86.6094,\n",
            "         -84.8940,  -94.3726,  -57.8222,  -57.8614,  -74.2751,  -96.8950,\n",
            "         -80.5040,  -84.5648,  -82.4602,  -87.8758, -112.3007,  -93.0688,\n",
            "        -114.1345,  -58.6394], device='cuda:0', grad_fn=<SumBackward2>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P-PXwfv0f22r",
        "colab_type": "code",
        "outputId": "a23f9dcf-24e9-4af1-a87b-fa5717ef1622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# question 2.2  \n",
        "#Evaluating likelihood using IWAE on test set \n",
        "model.eval()\n",
        "t_loss=0\n",
        "for batch_index, data in enumerate(test_loader):\n",
        "    data = data.to(device)\n",
        "    reconstruction_batch, mu, log_variance = model(data)\n",
        "\n",
        "    loss= model.evaluate_IWAE(200,data,reconstruction_batch , mu , log_variance) \n",
        "    t_loss+=loss.item()\n",
        "    \n",
        "print('Likelihod on whole Test set is : ', t_loss/len(test_loader.dataset)) \n",
        "\n",
        "    \n",
        "#Evaluating likelihood using IWAE on validation set         \n",
        "model.eval()\n",
        "t_loss=0\n",
        "for batch_index, data in enumerate(valid_loader):\n",
        "    data = data.to(device)\n",
        "    reconstruction_batch, mu, log_variance = model(data)\n",
        "\n",
        "    loss= model.evaluate_IWAE(200,data,reconstruction_batch , mu , log_variance) \n",
        "    t_loss+=loss.item()\n",
        "     \n",
        "print('Likelihod on whole Validation set is : ', t_loss/len(valid_loader.dataset))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Likelihod on whole Test set is :  -85.09010629882812\n",
            "Likelihod on whole Validation set is :  -85.81766625976563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IeSAtzn5C1Uq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}